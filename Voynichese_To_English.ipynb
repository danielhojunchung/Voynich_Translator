{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf17l_pNNUJh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(\n",
        "    texts: list[str],\n",
        "    max_length: int = 128,\n",
        "    num_beams: int = 4,\n",
        "    do_sample: bool = False\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      texts:       list of raw input strings in the source language\n",
        "      max_length:  max tokens in the generated output\n",
        "      num_beams:   beam size (set to 1 for greedy)\n",
        "      do_sample:   whether to sample or not\n",
        "\n",
        "    Returns:\n",
        "      List of translated strings\n",
        "    \"\"\"\n",
        "    # Tokenize inputs\n",
        "    enc = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    ).to(device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        out_ids = model.generate(\n",
        "            input_ids=enc.input_ids,\n",
        "            attention_mask=enc.attention_mask,\n",
        "            max_length=max_length,\n",
        "            num_beams=num_beams,\n",
        "            do_sample=do_sample,\n",
        "            early_stopping=True,\n",
        "            decoder_start_token_id=model.config.decoder_start_token_id,\n",
        "            pad_token_id=model.config.pad_token_id,\n",
        "            eos_token_id=model.config.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode to strings\n",
        "    return tokenizer.batch_decode(out_ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "FHwSj3FNNUwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DIR = \"./my_translation_model\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
        "model     = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR).to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "NDWS1L02QSpS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}